{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b465d3fb",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a comprehensive machine learning pipeline for predicting loan approval status. The system analyzes applicant information to determine whether a loan should be approved or rejected using various machine learning algorithms.\n",
    "\n",
    "### Key Features:\n",
    "- **Data Preprocessing**: Comprehensive data cleaning and feature engineering\n",
    "- **Model Comparison**: Multiple ML algorithms evaluation\n",
    "- **Hyperparameter Tuning**: Automated optimization for best performance\n",
    "- **Model Persistence**: Save and load trained models\n",
    "- **API Integration**: FastAPI-based REST API for real-time predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71895df8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "First, we'll import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Custom modules\n",
    "from src.data_preprocessing import DataPreprocessor\n",
    "from src.model_training import ModelTrainer\n",
    "from src.utils import create_sample_data, decode_prediction, print_data_info\n",
    "from src.config import DATA_FILE, MODEL_FILE, SCALER_FILE\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58359a53",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "Let's load the dataset and explore its structure to understand the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Load the dataset\n",
    "df = preprocessor.load_data(DATA_FILE)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"üìã Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive data information\n",
    "print_data_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38188256",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now we'll clean the data, handle missing values, and prepare it for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before preprocessing\n",
    "print(\"‚ùå Missing Values (Before Preprocessing):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = preprocessor.handle_missing_values(df)\n",
    "\n",
    "# Check missing values after preprocessing\n",
    "print(\"\\n‚úÖ Missing Values (After Preprocessing):\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "print(f\"\\nüìä Data shape after cleaning: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore unique values in categorical columns\n",
    "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']\n",
    "\n",
    "print(\"üè∑Ô∏è Unique Values in Categorical Columns:\")\n",
    "for col in categorical_columns:\n",
    "    if col in df_clean.columns:\n",
    "        unique_values = df_clean[col].unique()\n",
    "        print(f\"{col}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "df_encoded = preprocessor.encode_categorical_variables(df_clean)\n",
    "\n",
    "print(\"üîÑ Categorical Variables Encoded:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee39098",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Let's visualize the data to gain insights before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58371653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Loan Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loan Status Distribution\n",
    "loan_status_counts = df_clean['Loan_Status'].value_counts()\n",
    "axes[0, 0].pie(loan_status_counts.values, labels=['Approved (Y)', 'Rejected (N)'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Loan Status Distribution')\n",
    "\n",
    "# 2. Income Distribution\n",
    "axes[0, 1].hist(df_clean['ApplicantIncome'], bins=30, alpha=0.7, label='Applicant Income')\n",
    "axes[0, 1].hist(df_clean['CoapplicantIncome'], bins=30, alpha=0.7, label='Coapplicant Income')\n",
    "axes[0, 1].set_title('Income Distribution')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Loan Amount vs Loan Status\n",
    "sns.boxplot(data=df_clean, x='Loan_Status', y='LoanAmount', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Loan Amount by Loan Status')\n",
    "\n",
    "# 4. Education vs Loan Status\n",
    "education_loan = pd.crosstab(df_clean['Education'], df_clean['Loan_Status'])\n",
    "education_loan.plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Education vs Loan Status')\n",
    "axes[1, 1].set_xlabel('Education')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend(['Rejected', 'Approved'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225ed6c",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Data Splitting\n",
    "\n",
    "Now we'll prepare the features and target variable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X, y = preprocessor.prepare_features_target(df_encoded)\n",
    "\n",
    "print(\"üéØ Features and Target Separated:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Display target distribution\n",
    "print(f\"\\nüìä Target Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Approval rate: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95036e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "X_scaled = preprocessor.scale_numerical_features(X, fit_scaler=True)\n",
    "\n",
    "print(\"‚öñÔ∏è Numerical Features Scaled:\")\n",
    "print(\"Numerical columns scaled:\", preprocessor.numerical_columns)\n",
    "print(\"\\nScaled features preview:\")\n",
    "print(X_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e540f",
   "metadata": {},
   "source": [
    "## 6. Model Training and Comparison\n",
    "\n",
    "We'll train multiple machine learning models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284def2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "print(\"ü§ñ Available Models:\")\n",
    "for name in trainer.models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "\n",
    "print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Starting model comparison...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1661a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "model_scores = trainer.compare_models(X_scaled, y)\n",
    "\n",
    "# Create visualization of model performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = list(model_scores.keys())\n",
    "scores = list(model_scores.values())\n",
    "\n",
    "bars = plt.bar(models, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Cross-Validation Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add score labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4425d",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "We'll optimize the best performing models using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from comparison\n",
    "best_model_name = max(model_scores, key=model_scores.get)\n",
    "print(f\"üèÜ Best Model: {best_model_name} (Score: {model_scores[best_model_name]:.4f})\")\n",
    "\n",
    "# Tune hyperparameters for top models\n",
    "models_to_tune = ['Random Forest', 'Logistic Regression', 'Support Vector Machine']\n",
    "\n",
    "tuned_models = {}\n",
    "for model_name in models_to_tune:\n",
    "    if model_name in trainer.models:\n",
    "        print(f\"\\nüîß Tuning {model_name}...\")\n",
    "        tuned_model = trainer.tune_hyperparameters(model_name, X_scaled, y)\n",
    "        tuned_models[model_name] = tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f1855",
   "metadata": {},
   "source": [
    "## 8. Final Model Selection and Evaluation\n",
    "\n",
    "Let's select the best model and evaluate its performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best tuned model (assuming Random Forest performed best)\n",
    "final_model = tuned_models.get('Random Forest', trainer.models['Random Forest'])\n",
    "\n",
    "print(\"üéñÔ∏è Final Model Selected: Random Forest\")\n",
    "print(\"\\nüìä Detailed Model Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get detailed evaluation\n",
    "evaluation = trainer.get_detailed_evaluation(final_model, X_scaled, y)\n",
    "\n",
    "print(f\"Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(evaluation['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "conf_matrix = evaluation['confusion_matrix']\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Rejected', 'Approved'],\n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display additional metrics\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nüìà Additional Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1332",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for loan approval prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfef19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_scaled.columns,\n",
    "        'importance': final_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Feature Importance Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    \n",
    "    # Add importance values on bars\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance['feature'], feature_importance['importance'])):\n",
    "        plt.text(importance + 0.002, i, f'{importance:.3f}', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üîç Top 5 Most Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance.head().values):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b915e",
   "metadata": {},
   "source": [
    "## 10. Model Persistence\n",
    "\n",
    "Save the trained model and preprocessing objects for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "trainer.save_model(final_model, MODEL_FILE)\n",
    "\n",
    "# Save the scaler\n",
    "from src.utils import save_preprocessing_objects\n",
    "save_preprocessing_objects(preprocessor.scaler, SCALER_FILE)\n",
    "\n",
    "print(\"üíæ Model and preprocessing objects saved successfully!\")\n",
    "print(f\"üìÅ Model saved to: {MODEL_FILE}\")\n",
    "print(f\"üìÅ Scaler saved to: {SCALER_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fb025",
   "metadata": {},
   "source": [
    "## 11. Model Testing with Sample Data\n",
    "\n",
    "Let's test our model with sample data to ensure it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample data\n",
    "print(\"üß™ Testing Model with Sample Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test approved case\n",
    "approved_sample = create_sample_data(\"approved\")\n",
    "approved_processed = preprocessor.preprocess_prediction_data(approved_sample)\n",
    "approved_prediction = final_model.predict(approved_processed)\n",
    "\n",
    "print(\"‚úÖ Sample Case 1 (Expected: Approved)\")\n",
    "print(f\"Input: {approved_sample.iloc[0].to_dict()}\")\n",
    "print(f\"Prediction: {decode_prediction(approved_prediction[0])}\")\n",
    "\n",
    "# Test rejected case\n",
    "rejected_sample = create_sample_data(\"rejected\")\n",
    "rejected_processed = preprocessor.preprocess_prediction_data(rejected_sample)\n",
    "rejected_prediction = final_model.predict(rejected_processed)\n",
    "\n",
    "print(\"\\n‚ùå Sample Case 2 (Expected: Rejected)\")\n",
    "print(f\"Input: {rejected_sample.iloc[0].to_dict()}\")\n",
    "print(f\"Prediction: {decode_prediction(rejected_prediction[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f73942",
   "metadata": {},
   "source": [
    "## 12. API Integration Testing\n",
    "\n",
    "Let's test our FastAPI integration to ensure the API works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API compatibility\n",
    "print(\"üåê API Integration Test:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Sample API request data\n",
    "api_request = {\n",
    "    \"Gender\": 1,\n",
    "    \"Married\": 1,\n",
    "    \"Dependents\": 0,\n",
    "    \"Education\": 1,\n",
    "    \"Self_Employed\": 0,\n",
    "    \"ApplicantIncome\": 5000,\n",
    "    \"CoapplicantIncome\": 2000,\n",
    "    \"LoanAmount\": 150,\n",
    "    \"Loan_Amount_Term\": 360,\n",
    "    \"Credit_History\": 1,\n",
    "    \"Property_Area\": 2\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and process\n",
    "api_df = pd.DataFrame([api_request])\n",
    "api_processed = preprocessor.preprocess_prediction_data(api_df)\n",
    "api_prediction = final_model.predict(api_processed)\n",
    "\n",
    "print(\"üìù API Request Format:\")\n",
    "print(api_request)\n",
    "print(f\"\\nüéØ API Response: {{'Loan status': '{decode_prediction(api_prediction[0])}'}}\")\n",
    "\n",
    "print(\"\\n‚úÖ API integration test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44addc80",
   "metadata": {},
   "source": [
    "## 13. Project Summary\n",
    "\n",
    "Let's summarize the key findings and results of our loan approval prediction project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä PROJECT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìÅ Dataset: {DATA_FILE}\")\n",
    "print(f\"üìà Dataset Size: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"üéØ Target Variable: Loan_Status (Approval Rate: {y.mean():.2%})\")\n",
    "\n",
    "print(f\"\\nü§ñ Models Evaluated: {len(trainer.models)}\")\n",
    "for name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  ‚Ä¢ {name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {type(final_model).__name__}\")\n",
    "print(f\"üéØ Final Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "print(f\"üìä Precision: {precision:.4f}\")\n",
    "print(f\"üìä Recall: {recall:.4f}\")\n",
    "print(f\"üìä F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    top_feature = feature_importance.iloc[0]['feature']\n",
    "    top_importance = feature_importance.iloc[0]['importance']\n",
    "    print(f\"\\nüîç Most Important Feature: {top_feature} ({top_importance:.4f})\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saved: {MODEL_FILE}\")\n",
    "print(f\"üíæ Scaler Saved: {SCALER_FILE}\")\n",
    "print(f\"üåê API Ready: app.py\")\n",
    "\n",
    "print(\"\\n‚úÖ Project completed successfully!\")\n",
    "print(\"üöÄ Ready for deployment and portfolio showcase!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb41aa2",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "### For Production Deployment:\n",
    "1. **API Testing**: Test the FastAPI application with Postman or curl\n",
    "2. **Model Monitoring**: Implement monitoring for model drift\n",
    "3. **Data Validation**: Add input validation and error handling\n",
    "4. **Documentation**: Update API documentation with examples\n",
    "5. **Containerization**: Create Docker container for easy deployment\n",
    "\n",
    "### For Portfolio Enhancement:\n",
    "1. **Visualizations**: Create interactive dashboards with Plotly/Dash\n",
    "2. **Web Interface**: Build a user-friendly web interface\n",
    "3. **Model Explainability**: Add SHAP or LIME explanations\n",
    "4. **A/B Testing**: Implement multiple model comparison\n",
    "5. **Real-time Predictions**: Add streaming prediction capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook demonstrates a complete end-to-end machine learning pipeline suitable for production use and portfolio showcase.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
